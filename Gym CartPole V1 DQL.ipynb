{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import numpy as np \n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c8d10",
   "metadata": {},
   "source": [
    "# Task 1 : \n",
    "Add a third action to the possible actions where the agent does not do anything, not pushing, not pulling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe79e6a",
   "metadata": {},
   "source": [
    "- Creation of the CustomCartPole class that inherites from the gym environment\n",
    "- Method overrinding to modify the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db043a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCartPole(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        \n",
    "        self.action_space = spaces.Discrete(3) # Add a third action\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.reward_range = self.env.reward_range\n",
    "        self.metadata = self.env.metadata \n",
    "        \n",
    "        self.last_obs = np.asarray(self.env.state)\n",
    "        self.last_reward = 0.0\n",
    "        self.last_term = False\n",
    "        self.last_trun = False\n",
    "        self.last_info = {}\n",
    "        \n",
    "    def step(self, action): # modifying the step method \n",
    "        \n",
    "        # If action is 2, do nothing\n",
    "        if action == 2:\n",
    "            \n",
    "            self.last_reward = 0.0\n",
    "            self.last_term = False\n",
    "            self.last_trun = False\n",
    "            self.last_info = {}\n",
    "            \n",
    "        else:\n",
    "            # Use the default action space (0=move left, 1=move right)\n",
    "            self.last_obs, self.last_reward, self.last_term, self.last_trun, self.last_info = self.env.step(action)\n",
    "        \n",
    "        return self.last_obs, self.last_reward, self.last_term, self.last_trun, self.last_info \n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd7663",
   "metadata": {},
   "source": [
    "# Task 2:\n",
    "Find a deep RL solution for your new environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15f8ec",
   "metadata": {},
   "source": [
    "- I chose to use Deep Q-learning (Instead of using a Q-table, we use a Neural Network that takes a state and approximates the Q-values for each action based on that state )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ceeb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent(): \n",
    "    \n",
    "    def __init__(self, env):\n",
    "        # parameters and hyperparameters\n",
    "        \n",
    "        # this part is for neural network or build_model()\n",
    "        self.state_size = env.observation_space.shape[0] # this variable contain number of states (for the input layer)\n",
    "        self.action_size = env.action_space.n # this cariable contains the number of action (for the output layer)\n",
    "        # this part is for replay()\n",
    "        self.gamma = 0.95\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # this part is for adaptiveEGreedy()\n",
    "        self.epsilon = 1 # initial exploration rate\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.memory = deque(maxlen = 10_000) # a list with 10000 memory, if it becomes full first inputs will be deleted\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # neural network for deep Q-learning\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = self.state_size, activation = 'relu')) # first hidden layer\n",
    "        model.add(Dense(24, activation='relu')) # second hidden layer\n",
    "        model.add(Dense(self.action_size, activation = 'linear')) # output layer        \n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, termination):\n",
    "        # storage\n",
    "        self.memory.append((state, action, reward, next_state, termination))\n",
    "    \n",
    "    def act(self, state):\n",
    "        # acting, exploit or explore\n",
    "        if random.uniform(0,1) <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            return np.argmax(act_values[0])\n",
    "            \n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        # training\n",
    "        \n",
    "        if len(self.memory) < batch_size:\n",
    "            return # memory is still not full\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size) # take 15 (batch_size) random samples from memory\n",
    "        for state, action, reward, next_state, termination in minibatch:\n",
    "            if termination: # if the game is over, I dont have next state, I just have reward \n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) \n",
    "                # target = R(s,a) + gamma * max Q`(s`,a`)\n",
    "                # target (max Q` value) is output of Neural Network which takes s` as an input \n",
    "                # amax(): flatten the lists (make them 1 list) and take max value\n",
    "            train_target = self.model.predict(state) # s --> NN --> Q(s,a)=train_target\n",
    "            train_target[0][action] = target\n",
    "            self.model.fit(state, train_target,epochs=1,verbose = 0) \n",
    "    \n",
    "    def adaptiveEGreedy(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initialize custom environment and agent\n",
    "    env = CustomCartPole()\n",
    "    \n",
    "    agent = DQLAgent(env)\n",
    "\n",
    "    batch_size = 15\n",
    "    episodes = 450\n",
    "    for e in range(episodes):\n",
    "        \n",
    "        # initialize environment\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        state = np.reshape(state, [1,4])\n",
    "        \n",
    "        total_reward = 0 # this variable will contain the total reward of each episode \n",
    "        x = 0 # Truncation: the game ends after 500 steps\n",
    "        while True:\n",
    "            \n",
    "            # act\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # step\n",
    "            next_state, reward, termination, _,_ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1,4])\n",
    "            \n",
    "            # add the reward of the step to the total\n",
    "            total_reward = total_reward+ reward\n",
    "            \n",
    "            # +1 every step and ends the episode when = 500\n",
    "            x=x+1\n",
    "            \n",
    "            # remember / storage\n",
    "            agent.remember(state, action, total_reward, next_state, termination)\n",
    "            \n",
    "            # update state\n",
    "            state = next_state\n",
    "            \n",
    "            # replay\n",
    "            agent.replay(batch_size)\n",
    "            \n",
    "            # adjust epsilon\n",
    "            agent.adaptiveEGreedy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            if termination or x>499:\n",
    "                print('Episode: {}, Reward: {}'.format(e, total_reward))\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
